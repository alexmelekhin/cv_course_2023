{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center>\n",
        "\n",
        "# [Компьютерное зрение](http://rairi.ru/wiki/index.php/%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5)\n",
        "\n",
        "## <center> Семинар 13 - 3D детекция с использованием лидарных облаков точек\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/alexmelekhin/cv_course_2023/blob/main/seminars/seminar_13/Seminar_13.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncoZ2PZ0_HJq"
      },
      "source": [
        "# Установка MMDetection3D\n",
        "\n",
        "Установка осуществляется согласно инструкции [Get Started](https://github.com/open-mmlab/mmdetection3d/blob/main/docs/en/get_started.md) из оригинального репозитория."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd_7zdj5_Bxq",
        "outputId": "5161f14a-a3d0-4315-fb64-0aaa73d91ec5"
      },
      "outputs": [],
      "source": [
        "!pip3 install openmim\n",
        "!mim install mmengine\n",
        "!mim install \"mmcv>=2.0.0rc4,<2.1.0\"\n",
        "!mim install \"mmdet>=3.0.0,<3.1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aAfpMPWF_qP5",
        "outputId": "0fababdf-1491-42aa-a35d-806167141e36"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/open-mmlab/mmdetection3d.git -b dev-1.x\n",
        "%cd mmdetection3d\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMLpbkY6_-IA",
        "outputId": "635bdd91-8474-406b-edf4-eba2a022da75"
      },
      "outputs": [],
      "source": [
        "import mmdet3d\n",
        "print(mmdet3d.__version__)\n",
        "# Example output: 1.1.0rc0, or an another version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKP_9jSRLnFY",
        "outputId": "6e81f318-fcfd-4b5c-8191-20601ec91a1c"
      },
      "outputs": [],
      "source": [
        "!mim download mmdet3d --config pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car --dest ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VoxelNet\n",
        "\n",
        "https://arxiv.org/abs/1711.06396\n",
        "\n",
        "> Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.\n",
        "\n",
        "![voxelnet.png](https://github.com/steph1793/Voxelnet/raw/master/images/pre.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v_mgT6IZb0Vq"
      },
      "source": [
        "# PointPillars\n",
        "\n",
        "https://github.com/open-mmlab/mmdetection3d/tree/main/configs/pointpillars \n",
        "\n",
        "![pointpillars.png](https://user-images.githubusercontent.com/79644370/143885905-aab6ffcf-7727-495e-90ca-edb8dd5e324b.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-OaIM6KAjTr",
        "outputId": "14a5610b-16b7-45a8-a335-123665d3a2a5"
      },
      "outputs": [],
      "source": [
        "from mmdet3d.apis import init_model, inference_detector\n",
        "\n",
        "config_file = 'pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py'\n",
        "checkpoint_file = 'hv_pointpillars_secfpn_6x8_160e_kitti-3d-car_20220331_134606-d42d15ed.pth'\n",
        "model = init_model(config_file, checkpoint_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnIZ_B_0oYqJ"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5k6mAUzydo"
      },
      "source": [
        "## Pillars Feature Net\n",
        "\n",
        "https://github.com/open-mmlab/mmdetection3d/blob/1.0/mmdet3d/models/voxel_encoders/pillar_encoder.py#L13\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdXSyF3y0H6h"
      },
      "source": [
        "## Backbone + neck - SECOND + FPN\n",
        "\n",
        "https://github.com/open-mmlab/mmdetection3d/blob/1.0/mmdet3d/models/backbones/second.py#L12\n",
        "\n",
        "https://github.com/open-mmlab/mmdetection3d/blob/1.0/mmdet3d/models/necks/second_fpn.py#L12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRvLxu_CiNV"
      },
      "source": [
        "## Head - Anchor3DHead\n",
        "\n",
        "https://github.com/open-mmlab/mmdetection3d/blob/1.0/mmdet3d/models/dense_heads/anchor3d_head.py#L16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAdF8xNwCq_l"
      },
      "source": [
        "# Инференс"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZE_wfE_aCdf",
        "outputId": "646d3787-d707-41ea-b480-6d0c3e1f00c3"
      },
      "outputs": [],
      "source": [
        "result, data = inference_detector(model, 'mmdetection3d/demo/data/kitti/000008.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"inputs\"].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgFeHRmvHeit",
        "outputId": "a7036511-4286-4600-8330-65145b81aae5"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8bzIEhAHigE",
        "outputId": "21fd8c63-2338-4739-fb78-a60f2300327b"
      },
      "outputs": [],
      "source": [
        "result.pred_instances_3d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5DlP1msJdbT"
      },
      "source": [
        "### LiDARInstance3DBoxes\n",
        "\n",
        "https://github.com/open-mmlab/mmdetection3d/blob/main/mmdet3d/structures/bbox_3d/lidar_box3d.py\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Each row is (x, y, z, x_size, y_size, z_size, yaw)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EFx_et5MZx_1"
      },
      "source": [
        "# Визуализация\n",
        "\n",
        "https://mmdetection3d.readthedocs.io/en/latest/user_guides/visualization.html\n",
        "\n",
        "! Не работает в Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwGle1TaHyPV"
      },
      "outputs": [],
      "source": [
        "from mmdet3d.visualization import Det3DLocalVisualizer\n",
        "\n",
        "points = data[\"inputs\"][\"points\"].numpy()\n",
        "\n",
        "visualizer = Det3DLocalVisualizer()\n",
        "\n",
        "# set point cloud in visualizer\n",
        "visualizer.set_points(points)\n",
        "bboxes_3d = result.pred_instances_3d.bboxes_3d\n",
        "visualizer.draw_bboxes_3d(bboxes_3d)\n",
        "visualizer.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Задание\n",
        "\n",
        "По примеру кода выше запустите инференс модели CenterPoint. Вы можете использовать любую конфигурацию из предложенных в MMDetection3D.\n",
        "\n",
        "В качестве входных данных используйте `mmdetection3d/demo/data/nuscenes/n015-2018-07-24-11-22-45+0800__LIDAR_TOP__1532402927647951.pcd.bin`.\n",
        "\n",
        "- https://github.com/open-mmlab/mmdetection3d/tree/main/configs/centerpoint\n",
        "- https://arxiv.org/abs/2006.11275\n",
        "\n",
        "\n",
        "### Вопрос 1\n",
        "\n",
        "Кратко опишите ключевую идею 2D CenterNet детектора.\n",
        "\n",
        "**Ответ:**\n",
        "\n",
        "### Вопрос 2\n",
        "\n",
        "Как осуществляется трекинг в CenterPoint?\n",
        "\n",
        "**Ответ:**\n",
        "\n",
        "### Вопрос 3\n",
        "\n",
        "Кратко опишите две стадии работы метода CenterPoint.\n",
        "\n",
        "**Ответ:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
